{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a851ec63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if not os.path.isdir(\"cld_optimization_experiments\"):\n",
    "    !git clone https://github.com/oopir/cld_optimization_experiments\n",
    "\n",
    "%cd cld_optimization_experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5732a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from src.data import load_1d_regression_data\n",
    "from src.training import get_1d_regression_curves_for_betas\n",
    "from src.plots import plot_1d_regression_curves\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bad3639",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_1d_regression_data(device=device)\n",
    "\n",
    "betas_to_plot = [1e07, 1e04, 1e02]\n",
    "seeds_for_curves = list(range(1))\n",
    "\n",
    "epochs = int(1e07)\n",
    "track_every = max(1,epochs//100) # track at most 100 times\n",
    "print_every = max(1, epochs//10)\n",
    "\n",
    "common = dict(\n",
    "    data=data,\n",
    "    eta=1e-7, \n",
    "    epochs=epochs,\n",
    "    lam_fc1=data[\"d_in\"] / (torch.nn.init.calculate_gain(\"tanh\") ** 2),\n",
    "    regularization_scale=1.0,\n",
    "    device=device,\n",
    "    print_every=print_every\n",
    ")\n",
    "\n",
    "x_plot    = torch.linspace(-1.5, 1.5, 400, device=device).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0c2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "curves_by_beta = get_1d_regression_curves_for_betas(\n",
    "    x_plot=x_plot,\n",
    "    betas=betas_to_plot,\n",
    "    seeds=seeds_for_curves,\n",
    "    common=common,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a73d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1d_regression_curves(data, x_plot, curves_by_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46638b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_and_return_model(beta, seed, data, eta, epochs, lam_fc1, lam_fc2, hidden_width, regularization_scale, device, print_every):\n",
    "#     torch.manual_seed(seed)\n",
    "#     torch.cuda.manual_seed_all(seed)\n",
    "#     np.random.seed(seed)\n",
    "#     random.seed(seed)\n",
    "\n",
    "#     X_train = data[\"X_train\"]\n",
    "#     y_train = data[\"y_train\"].unsqueeze(1)  # (N, 1)\n",
    "#     d_in = data[\"d_in\"]\n",
    "#     d_out = data[\"d_out\"]\n",
    "\n",
    "#     model = TwoLayerNet(d_in=d_in, hidden=hidden_width, d_out=d_out, with_bias=True).to(device)\n",
    "#     params, lam_tensors = make_lambda_like_params(model, lam_fc1, lam_fc2)\n",
    "\n",
    "#     for epoch in range(epochs):\n",
    "#         model.train()\n",
    "#         for p in params:\n",
    "#             if p.grad is not None:\n",
    "#                 p.grad.zero_()\n",
    "#         outputs = model(X_train)\n",
    "#         loss = loss_fn(outputs, y_train)\n",
    "#         loss.backward()\n",
    "#         langevin_step(params,lam_tensors,beta=beta,eta=eta,regularization_scale=regularization_scale)\n",
    "#         if epoch % print_every == 0:\n",
    "#             print(f\"    epoch = {epoch:5} | loss = {loss:.2}\")\n",
    "\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0993cec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_curves_for_betas(betas, seeds):\n",
    "#     curves = {}\n",
    "#     m_values = [int(min(1e05, beta * np.log(beta))) for beta in betas_to_plot]\n",
    "#     for beta, m in zip(betas, m_values):\n",
    "#         print(f\"beta={beta:.0e}, m={m:.2e}\")\n",
    "#         fs = []\n",
    "#         for seed in seeds:\n",
    "#             print(f\"  seed={seed}\")\n",
    "#             model = train_and_return_model(beta=beta, seed=seed, lam_fc2=m, hidden_width=m, **common)\n",
    "#             with torch.no_grad():\n",
    "#                 f = model(x_plot).cpu().numpy().ravel()\n",
    "#             fs.append(f)\n",
    "#         curves[beta] = np.stack(fs, axis=0)  # (n_seeds, n_grid)\n",
    "#     return curves\n",
    "\n",
    "# curves_by_beta = get_curves_for_betas(betas_to_plot, seeds_for_curves)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b5b1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_np = data[\"X_train\"].cpu().numpy().ravel()\n",
    "# y_train_np = data[\"y_train\"].cpu().numpy().ravel()\n",
    "# y_target_np = np.interp(x_plot_np, X_train_np, y_train_np)\n",
    "\n",
    "# plt.figure(figsize=(6, 4))\n",
    "\n",
    "# plt.plot(x_plot_np, y_target_np, \"k--\", label=\"target\")\n",
    "# plt.scatter(X_train_np, y_train_np, c=\"k\", s=20, zorder=3)\n",
    "\n",
    "# for beta, fs in curves_by_beta.items():\n",
    "#     mean = fs.mean(axis=0)\n",
    "#     std = fs.std(axis=0)\n",
    "#     label = f\"Î²={beta:.1e}\"\n",
    "#     plt.plot(x_plot_np, mean, label=label)\n",
    "#     plt.fill_between(x_plot_np, mean - std, mean + std, alpha=0.2)\n",
    "\n",
    "# plt.xlim(-1.5, 1.5)\n",
    "# plt.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
